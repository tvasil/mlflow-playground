{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "from so_tag_classifier_core import (text_prepare, binarize_ys, tokenize_and_stem, transform_y)\n",
    "\n",
    "import dill\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report, accuracy_score, average_precision_score, \n",
    "                             f1_score, precision_score, make_scorer)\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\") as f:\n",
    "    configs = yaml.safe_load(f)\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = configs.get(\"MLFLOW_TRACKING_USERNAME\")\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = configs.get(\"MLFLOW_TRACKING_PASSWORD\")\n",
    "\n",
    "TRACKING_URI = configs.get(\"TRACKING_URI\")\n",
    "BUCKET = configs.get(\"BUCKET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment('stackoverlow-classifier')\n",
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract conda env\n",
    "\n",
    "This environment file with later be passed on to the model definition directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_env = {\n",
    "            'name': 'so-classifier-env',\n",
    "            'channels': ['defaults', \"anaconda\", \"conda-firge\"],\n",
    "            'dependencies': [\n",
    "                'python=3.7.9',\n",
    "                'scikit-learn>=0.21.3',\n",
    "                'pip=20.3.1',\n",
    "                'setuptools=51.0.0',\n",
    "                {'pip': \n",
    "                 ['boto3==1.16.34',\n",
    "                  'cloudpickle==1.6.0',\n",
    "                  'dill==0.3.3',\n",
    "                  'mlflow=1.12.1',\n",
    "                  'nltk[stopwords]==3.5',\n",
    "                  'nltk[punkt]==3.5',\n",
    "                  'numpy==1.19.4',\n",
    "                  'pandas==1.1.5',\n",
    "                  'scikit-learn==0.23.2',\n",
    "                  'scikit-optimize==0.8.1',\n",
    "                  'scipy==1.5.4']\n",
    "                }\n",
    "            ]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Python class \n",
    "\n",
    "This custom class lets us extract multiple artifacts from the `model` registry, not just the `classifier`. Also, it allows us to define a custom inference function (`.predict`), which will also transform the output data from a 100-element long matrix to just the labels we want to predict, along with their corresponding probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelClassifierPipelineWrapper(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def load_context(self, context):\n",
    "        self.binarizer = dill.load(context.artifacts[\"binarizer\"])\n",
    "        self.pipeline = dill.load(context.artifacts[\"pipeline\"])\n",
    "        \n",
    "\n",
    "    def predict(self, context, document):\n",
    "        \"\"\"\n",
    "        Make a label prediction for an arbitrary number of documents/texts\n",
    "        \"\"\"\n",
    "        \n",
    "        vals = document.text.tolist()\n",
    "        raw_preds = self.pipeline.predict(vals)\n",
    "        preds = self.binarizer.inverse_transform(raw_preds)\n",
    "        \n",
    "        probs = self.pipeline.predict_proba(vals)\n",
    "        all_probs_dict = [dict(zip(self.binarizer.classes_, prob)) for prob in probs]\n",
    "        to_return = []\n",
    "        for pred, probs_dict in zip(preds, all_probs_dict):\n",
    "            to_return.append({x:probs_dict[x] for x in probs_dict if x in pred})\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(y_test, y_preds):\n",
    "    accuracy = accuracy_score(y_test, y_preds)\n",
    "    f1 = f1_score(y_test, y_preds, average=\"weighted\")\n",
    "    avg_precision = average_precision_score(y_test, y_preds)\n",
    "    precision = precision_score(y_test, y_preds, average=\"weighted\")\n",
    "    return {\"accuracy\": accuracy, \n",
    "            \"f1\": f1, \n",
    "            \"avg_precision\": avg_precision, \n",
    "            \"precision\": precision}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=\"/Users/tania/tvasil/stackoverflow-topic-classifier/data/full_body_clean.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file)\n",
    "#df = df.sample(10000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'].values, \n",
    "                                                    df['tags'].values, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "#X_train_cleaned = pd.Series(text_prepare(X_train, \" \"))\n",
    "binarizer, y_train_binarized, y_test_binarized = binarize_ys(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn(y_target, y_prediction):\n",
    "#     \"\"\"\n",
    "#     Custom loss function that maximizes the precision of the model\n",
    "#     \"\"\"\n",
    "#     return 1.0 - precision_score(y_target, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estim = HyperoptEstimator(classifier=one_vs_rest('clf', estimator=svc('estimator')),  \n",
    "#                           preprocessing=[tfidf('tfidf', lowercase=True)],\n",
    "#                           algo=tpe.suggest, \n",
    "#                           loss_fn=loss_fn,\n",
    "#                           trial_timeout=300, \n",
    "#                           verbose=True)\n",
    "# estim.fit(X_train_cleaned, y_train_binarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_search_space = {\n",
    "    \"tfidf__min_df\": Integer(5, 100),\n",
    "    \"tfidf__max_df\": Real(0.5, 0.99, prior='log-uniform'),\n",
    "    \"clf\": [ClassifierChain(LogisticRegression(random_state=42,\n",
    "                                                           dual=False, \n",
    "                                                           solver=\"liblinear\", \n",
    "                                                           max_iter=1000), \n",
    "                                             cv=3)],\n",
    "    \"clf__base_estimator__C\": Real(0.000001, 5e5, prior=\"uniform\"),\n",
    "    \"clf__base_estimator__penalty\": ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the search space and defaults \n",
    "estimators = [('preprocessor', FunctionTransformer(text_prepare, kw_args={'join_symbol': ' '})), \n",
    "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_and_stem, \n",
    "                                        ngram_range=(1, 3),\n",
    "                                        norm='l2')),\n",
    "              ('clf', ClassifierChain(LogisticRegression()))\n",
    "             ]\n",
    "\n",
    "# search_space = {\"tfidf__min_df\": np.arange(5, 100),\n",
    "#                 \"tfidf__max_df\": np.arange(0.01, 0.98, step=0.01),\n",
    "#                 \"clf\": [ClassifierChain(LogisticRegression(random_state=42,\n",
    "#                                                            dual=False, \n",
    "#                                                            max_iter=1000), \n",
    "#                                              cv=3)], \n",
    "#                 \"clf__base_estimator__C\": np.arange(0.000001, 50000, step=1),\n",
    "#                 \"clf__base_estimator__penalty\": ['l1', 'l2']}\n",
    "\n",
    "scoring = {'f1': make_scorer(f1_score, average= 'weighted'), \n",
    "           'average_precision': 'average_precision'}\n",
    "\n",
    "### Create the Pipeline and RSCV objects \n",
    "training_pipe = Pipeline(estimators, verbose=True)\n",
    "hyperoptsearch = BayesSearchCV(training_pipe,\n",
    "                          #param_grid=search_space,\n",
    "                          search_spaces=bayes_search_space, \n",
    "                          scoring=make_scorer(f1_score, average= 'weighted'),\n",
    "                          refit=True, \n",
    "                          return_train_score=True, \n",
    "                          cv=3, \n",
    "                          verbose=10, \n",
    "                          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  7.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  7.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"Second attempt at BayesSearch\") as run:\n",
    "    hyperoptsearch.fit(X_train, y_train_binarized)\n",
    "    signature = infer_signature(X_train, hyperoptsearch.best_estimator_.predict(X_train))\n",
    "    print(\"Logged data and model in run: {}\".format(run.info.run_id))\n",
    "    \n",
    "    ## CAPTURE METRICS\n",
    "    y_test_pred_binarized = hyperoptsearch.best_estimator_.predict(X_test)\n",
    "    class_report = classification_report(\n",
    "                                y_test_binarized, \n",
    "                                y_test_pred_binarized, \n",
    "                                target_names=binarizer.classes_, \n",
    "                                zero_division=1\n",
    "                            )\n",
    "    metrics = eval_metrics(y_test_binarized, y_test_pred_binarized)\n",
    "\n",
    "#     mlflow.log_params(rs.named_steps) # log pipeline steps -- could be improved\n",
    "#     mlflow.log_params(params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    \n",
    "    ## CREATE AND SAVE ARTIFACTS\n",
    "    \n",
    "    pipeline_path = \"models/classifier.pkl\"\n",
    "    binarizer_path = \"models/binarizer.pkl\"\n",
    "    class_report_path = \"metrics/classification_report.txt\"\n",
    "    \n",
    "    dill.dump(bscv.best_estimator_, pipeline_path)\n",
    "    dill.dump(binarizer, binarizer_path)\n",
    "    with open(class_report_path, 'w') as f:\n",
    "        f.write(class_report)\n",
    "\n",
    "    artifacts = {\n",
    "        \"pipeline\": pipeline_path,\n",
    "        \"binarizer\": binarizer_path,\n",
    "        \"classification_report\": class_report_path\n",
    "    }\n",
    "\n",
    "    mlflow_pyfunc_model_path = \"so_pyfunc_model\"\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=mlflow_pyfunc_model_path, \n",
    "        python_model=MultiLabelClassifierPipelineWrapper(), \n",
    "        artifacts=artifacts,\n",
    "        conda_env=conda_env, \n",
    "        signature=signature\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_s3 = \"XXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_path_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = pd.DataFrame(data={\"text\": [\"I can't figure out how to load a custom model from Tensorflow into my Python function\",\n",
    "                                         \"How to compile the Kotlin code\"]}, \n",
    "                          index=[0, 1])\n",
    "to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict.to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.predict(data=to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlflow]",
   "language": "python",
   "name": "conda-env-mlflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
